{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**\n",
    "\n",
    "Brain recordings, such as EEG and fMRI seek to characterize brain activity and connectivity. A complete understanding of the braib dynamics can lead to a much better understanding of brain pathologies and can help direct new paths into AI. Recordings seek to determine functional connectivity between the different hierarchial organizations of the brain, and this ideal representation of the brain is very important, because it can be directly mapped to brain physiology and it can directly inform us of the dynamics residing in the brain structure. \n",
    "\n",
    "Specifically, $ \\begin{equation} S = M(E(B(N,I,t))) \\end{equation}$ , where S is our recorded signal, B is the functional representation of the brain, dependent on N, the brain network, I, the input to the brain, and t, time. E is representing the mapping from the functional representation to the actual electrophysiology that encodes that information. M is representing the mapping from brain electrophysiology to the measurements achieved by recording devices. \n",
    "\n",
    "directed Brain Network $ \\begin{equation} N = (\\vec{v},\\vec{e}) \\end{equation}$, where v correspond to vertice states, and e correspond to edge functions between vertices. v is multidimensional, can represent many hierarchial levels in the brain, such as neurons, nerve bundles, and whole regions. For neurons, the state could take into many factors, for example, location, topology of the neuron, phase/frequency of firing, number of synaptic vesicles etc. \n",
    "\n",
    "Brain Dynamics B: $ \\begin{equation} \\dot{v_{i}} = g(I) + \\sum_{e \\in N}{e(v_{j},v_{i},t)} \\end{equation}$\n",
    "\n",
    "\\*Note that there can be self directed loops where edge function = $ \\begin{equation} e(v_{i},t) \\end{equation}$\n",
    "\n",
    "The Input signal can encapsulate input signal from the environment and internal signaling. Signals from the environement can be more directly measured (visual cues, sensory features etc.) and internal signaling is less direct, but both contain some noise and lost information.\n",
    "\n",
    "Electrophysiology E(B) maps the network state into a corresponding voltage density topology. Several electrophysiology models already exist to predict mapping of brain state to electrophysiology.\n",
    "\n",
    "M(E) is takes the measurements from the eletrophysiology. Almost total information is known about M, since it's fairly easy to measure quantities about M (location of electrodes, orientation etc.) and there is much prior information on M (hardware features, effects of filtering, postprocessing signal etc.). However this is where noise gets introduced, as devices can pick up un extraneuous or faulty information sources. \n",
    "\n",
    "Since the mapping from the functional brain to electrophysiology is fairly direct, and the functional network intrinsically contains much of the information of the brain system, the general Bayesian problem is as follows:\n",
    "\n",
    "*Given S,I, find N.*\n",
    "\n",
    "However, this is a very hard problem to solve, as S and I are relatively low dimensional signals compared to N. This problem can be reworded into:\n",
    "\n",
    "*Given S,I, find N' where arg min C(N,N')* \n",
    "\n",
    "C is our cost function associated with our predicted and actual N , which can encapsulate closeness of behavior of the two systems. Since biological mechanisms are usually repetitive and fractal-like, giving rise to complex structures with relatively simple rulesets and the brain can be globally modulated by input signals such as drugs, DBS, and rTMS, there may be some low dimensional features that can be extracted from S that can be used to predict the behavior of N to relatively high degree.\n",
    "\n",
    "There already exist multitudes of signal features that the current literature utilizes to ascertain certain features of N, but efficacy of these features to predict functional behavior is quantitaively uncertain, and should be studies further to determine relations between the detected features and underlying brain network behavior. Underlied here is a path to quantitavely determine upper bounds on these features in describing functional network behavior.\n",
    "\n",
    "A simple, ideal brain network B(N) with I = 0, and the network being time-invariant, was contructed. Then E was assumed to be identity functions, M was assumed to be a linear function and S was the actual signal from the time series of $ \\begin{equation} \\vec{v} \\end{equation}$. Several measures curently used in the literatrure were then tested in a Monte Carlo simulation to get upper bounds of the efficacy. The constructed model is as follows:\n",
    "\n",
    "$ \\begin{equation}\n",
    "\\dot{v_{i}} = g(v_{i}) + \\sum_{e \\in N}{f(v_{j},v_{i})}\n",
    "\\end{equation}$\n",
    "\n",
    "$ \\begin{equation}\n",
    "S = M \\vec{v} + N\n",
    "\\end{equation}$\n",
    ", where N is noise\n",
    "\n",
    "Network: All-to-All (K), Small-World (SW), Preferential Attachment (BA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Measures**\n",
    "\n",
    "CFC: PPC, AAC, PAC\n",
    "\n",
    "Coherence: CTC, PLV\n",
    "\n",
    "Spectral: RDP, Edge, Entropy, Moment\n",
    "\n",
    "Nonlinear: Correlation Dimension/Saturation, Largest Lyapunov exponent, Kolmogorof entropy\n",
    "\n",
    "Mutual Entropy: CMI, AMI\n",
    "\n",
    "Connectivity Measures: TE, GCI, PLI, Pairwise Phase Consistency, GC, DT, PDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import cmath as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, G, x, h, f, M, N, dt = .05):\n",
    "        self.G = G #Graph representation of network\n",
    "        self.x = x #states\n",
    "        self.h = h #node function\n",
    "        self.f = f #coupling function\n",
    "        self.M = M #measurement matrix\n",
    "        self.N = N #Variance for Gaussian noise\n",
    "        self.y = self.linear_measure() #measurement vectors\n",
    "        self.t = 0 #time\n",
    "        self.dt = dt #time step\n",
    "        \n",
    "        #checks\n",
    "        if len(self.x) == 0:\n",
    "            raise ValueError('self.x can\\'t have less than one state')\n",
    "        if len(self.x) != nx.number_of_nodes(self.G):\n",
    "            raise ValueError('length of self.x must match number of nodes')\n",
    "        if len(self.x) != len(self.M):\n",
    "            raise ValueError('length of self.x must match number of nodes')\n",
    "        \n",
    "    #state derivative\n",
    "    def dev(self,phase):\n",
    "        dev = np.matrix(np.zeros(len(self.x))).T      \n",
    "        for i in range(1,len(self.x)):\n",
    "            sumEdge = 0\n",
    "            if self.G[i]:\n",
    "                for j in self.G[i]:\n",
    "                    sumEdge += self.f(self.x[i,-1],self.x[(j-1),-1])\n",
    "            dev[i] = self.h(self.x[i,-1]) + sumEdge\n",
    "        return dev\n",
    "            \n",
    "    #linear measurement\n",
    "    def linear_measure(self):\n",
    "        return self.M * self.x[:,-1] + np.matrix(np.random.normal(0,self.N,len(self.x))).T\n",
    "    \n",
    "    #euler method approximation of behavior\n",
    "    def euler_step(self):\n",
    "        new_state = self.x[:,-1] + self.dev(self.x[:,-1])*self.dt\n",
    "        self.t += self.dt\n",
    "        self.x = np.hstack((self.x,new_state))\n",
    "        self.y = np.hstack((self.y,self.linear_measure()))\n",
    "       \n",
    "    #runge-Kutta approximation of behavior\n",
    "    def runge_kutta_step(self):\n",
    "        k1 = self.dev(self.x[:,-1])*self.dt\n",
    "        k2 = self.dev(self.x[:,-1]+ .5*k1)*self.dt\n",
    "        k3 = self.dev(self.x[:,-1]+ .5*k2)*self.dt\n",
    "        k4 = self.dev(self.x[:,-1]+ k3)*self.dt\n",
    "        new_state = self.x[:,-1] + (k1+ 2*k2 + 2*k3 + k4)/6\n",
    "        self.t += self.dt\n",
    "        self.x = np.hstack((self.x,new_state))\n",
    "        self.y = np.hstack((self.y,self.linear_measure(new_state)))\n",
    "    \n",
    "    #time step function\n",
    "    def step(self):\n",
    "        self.euler_step()\n",
    "        \n",
    "    #runs model and stores states\n",
    "    def run(self,T):\n",
    "        for ts in range(0,int(T/self.dt)):\n",
    "            self.step()\n",
    "            \n",
    "    #clears all states exept initial\n",
    "    def clear_run(self):\n",
    "        self.x = self.x[:,0]\n",
    "        self.y = self.y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create specific networks\n",
    "n = 10\n",
    "\n",
    "K = nx.complete_graph(n)\n",
    "SW = nx.watts_strogatz_graph(n, k = 2, p = .1)\n",
    "BA = nx.barabasi_albert_graph(n, m = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "x = np.matrix([10,1,10,1,1,10]).T\n",
    "\n",
    "nodes = [1,2,3,4,5,6]\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from([(1,2),(2,3),(1,6),(1,3),(3,4),(4,5),(4,6),(5,6)])\n",
    "\n",
    "def h(x):\n",
    "    return 0\n",
    "def f(x,y):\n",
    "    return y-x\n",
    "\n",
    "M = np.identity(6)\n",
    "N = .1\n",
    "\n",
    "model = Model(G,x,h,f,M,N)\n",
    "model.run(T = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Visualization Tools\n",
    "\n",
    "def state_course(states):\n",
    "    plt.figure()\n",
    "    plt.plot(states.T)\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('x')\n",
    "    plt.title('t')\n",
    "    plt.show()\n",
    "    \n",
    "def spectrogram(signal, sr = int(1/model.dt)):\n",
    "    f, t, s = sig.spectrogram(signal, sr)\n",
    "    plt.pcolormesh(t, f, s)\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.show()\n",
    "\n",
    "def PSD(signal, sr = int(1/model.dt)):\n",
    "    f, Pxx = sig.welch(signal, sr)\n",
    "    plt.semilogy(f, Pxx)\n",
    "    plt.xlabel('frequency [Hz]')\n",
    "    plt.ylabel('PSD [V**2/Hz]')\n",
    "    plt.show()\n",
    "    \n",
    "#state_course(model.x)\n",
    "#n = 1\n",
    "#spectrogram(model.y[n,:].tolist()[0])\n",
    "#PSD(model.y[n,:].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#runs functions across all signals\n",
    "def cross_func(sig, func):\n",
    "    M = np.zeros((len(sig), len(sig)))\n",
    "    for i in range(len(sig)):\n",
    "        for j in range(len(sig)):\n",
    "            M[i,j] = func(sig[i,:],sig[j,:])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherence\n",
    "coh = sig.coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation\n",
    "cor = sig.correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PLV\n",
    "def phase_locking_value(x,y):\n",
    "    complex_phase_diff = np.exp(np.complex(0,1)*(theta1 - theta2))\n",
    "    plv = np.abs(np.sum(complex_phase_diff))/len(theta1)\n",
    "    return plv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/metrics/cluster/supervised.py#L531\n",
    "#Mutual Information\n",
    "\n",
    "def mutual_info_score(labels_true, labels_pred, contingency=None):\n",
    "    if contingency is None:\n",
    "        labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "        contingency = contingency_matrix(labels_true, labels_pred, sparse=True)\n",
    "    else:\n",
    "        contingency = check_array(contingency,\n",
    "                                  accept_sparse=['csr', 'csc', 'coo'],\n",
    "                                  dtype=[int, np.int32, np.int64])\n",
    "\n",
    "    if isinstance(contingency, np.ndarray):\n",
    "        # For an array\n",
    "        nzx, nzy = np.nonzero(contingency)\n",
    "        nz_val = contingency[nzx, nzy]\n",
    "    elif sp.issparse(contingency):\n",
    "        # For a sparse matrix\n",
    "        nzx, nzy, nz_val = sp.find(contingency)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported type for 'contingency': %s\" %\n",
    "                         type(contingency))\n",
    "\n",
    "    contingency_sum = contingency.sum()\n",
    "    pi = np.ravel(contingency.sum(axis=1))\n",
    "    pj = np.ravel(contingency.sum(axis=0))\n",
    "    log_contingency_nm = np.log(nz_val)\n",
    "    contingency_nm = nz_val / contingency_sum\n",
    "    # Don't need to calculate the full outer product, just for non-zeroes\n",
    "    outer = pi.take(nzx) * pj.take(nzy)\n",
    "    log_outer = -np.log(outer) + log(pi.sum()) + log(pj.sum())\n",
    "    mi = (contingency_nm * (log_contingency_nm - log(contingency_sum)) +\n",
    "          contingency_nm * log_outer)\n",
    "    return mi.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nonlinear measures\n",
    "\n",
    "#import nolds.sampen as as sp\n",
    "#import nolds.corr_dim as cd\n",
    "#import nolds.lyap_r as lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Granger Causality\n",
    "\n",
    "#import nitime.analysis.granger as GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
